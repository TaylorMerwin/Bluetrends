services:
  broker:
    image: apache/kafka:latest
    container_name: broker
    hostname: broker
    ports:
      - "29092:29092"
      - "9092:9092"
      - "9093:9093"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: EXTERNAL://0.0.0.0:29092,INTERNAL://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: EXTERNAL://localhost:29092,INTERNAL://broker:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: EXTERNAL:PLAINTEXT,INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@broker:9093"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_PARTITIONS: 3
    volumes:
      - kafka-data:/var/lib/kafka/data

  python:
    build:
      context: .
      dockerfile: python/Dockerfile
    container_name: python
    hostname: python
    volumes:
      - ./src/ingestion:/home/app
    working_dir: /home/app
    command: sleep infinity
    depends_on:
      - broker

  airflow:
    image: apache/airflow:2.10.5
    container_name: airflow
    volumes:
      - ./src/airflow:/opt/airflow/dags # DAG files go in the airflow directory
      - ./data/airflow:/usr/local/airflow # Persist metadata if needed
    ports:
      - "8080:8080"
    command: webserver
    depends_on:
      - broker
      - python

  spark:
    image: bitnami/spark:3.5
    container_name: spark
    environment:
      - SPARK_MODE=master
    ports:
      - "8081:8080" # Spark UI on 8081 (or adjust accordingly)
    volumes:
      - ./src/processing:/opt/spark-apps # PySpark scripts live here
    depends_on:
      - broker

volumes:
  kafka-data:
