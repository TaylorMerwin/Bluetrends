# docker-compose.yml
services:
  broker:
    image: apache/kafka:latest
    container_name: broker
    hostname: broker
    ports:
      - "29092:29092"
      - "9092:9092"
      - "9093:9093"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: EXTERNAL://0.0.0.0:29092,INTERNAL://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: EXTERNAL://localhost:29092,INTERNAL://broker:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: EXTERNAL:PLAINTEXT,INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@broker:9093"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_PARTITIONS: 3
    volumes:
      - kafka-data:/var/lib/kafka/data

  python:
    build:
      context: .
      dockerfile: python/Dockerfile
    container_name: python
    hostname: python
    volumes:
      - ./src/ingestion:/home/app
    working_dir: /home/app
    command: sleep infinity
    depends_on:
      - broker

  airflow:
    image: apache/airflow:2.10.5
    container_name: airflow
    volumes:
      - ./src/airflow:/opt/airflow/dags # DAG files go in the airflow directory
      - ./data/airflow:/usr/local/airflow # Persist metadata if needed
    ports:
      - "8080:8080"
    command: webserver
    environment:
      - AIRFLOW_CONN_MYSQL_MOVIES=mysql://${MYSQL_USER}:${MYSQL_PASSWORD}@mysql:3306/${MYSQL_DATABASE}
    depends_on:
      - broker
      - python

  spark-master:
    image: bitnami/spark:3.5
    container_name: spark-master
    build:
      context: .
      dockerfile: spark/Dockerfile
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master

      # The correct environment var name:
      - SPARK_EVENTLOG_ENABLED=true
      - SPARK_EVENTLOG_DIR=/tmp/spark-events

    ports:
      - "8081:8080"
    volumes:
      - ./src/processing:/opt/spark-apps
      - spark-events:/tmp/spark-events
    depends_on:
      - broker
    env_file:
      - ./.env

  spark-worker-1:
    image: bitnami/spark:3.5
    container_name: spark-worker-1
    build:
      context: .
      dockerfile: spark/Dockerfile
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
    env_file:
      - ./.env

  spark-worker-2:
    image: bitnami/spark:3.5
    container_name: spark-worker-2
    build:
      context: .
      dockerfile: spark/Dockerfile
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
    env_file:
      - ./.env

  db:
    image: mysql:9.2.0
    container_name: db
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
      MYSQL_DATABASE: ${MYSQL_DATABASE}
      MYSQL_USER: ${MYSQL_USERNAME}
      MYSQL_PASSWORD: ${MYSQL_PASSWORD}
    ports:
      - "3306:3306"
    volumes:
      - mysql-data:/var/lib/mysql
      - ./mysql/initdb:/docker-entrypoint-initdb.d
    restart: always

volumes:
  kafka-data:
  mysql-data:
  spark-events:
